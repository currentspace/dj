# API Worker Architecture: Rate-Limited Request Orchestration

## Overview

The API worker uses a **unified rate-limited orchestrator** to manage all external API calls,
ensuring we stay within Cloudflare Workers' 40 RPS (requests per second) limit for outbound
subrequests.

## Core Components

### 1. RateLimitedQueue (`utils/RateLimitedQueue.ts`)

Foundation: Token bucket algorithm with bounded concurrency

- **Rate limiting**: 40 tokens/second (configurable)
- **Burst capacity**: Allows bursts up to `burst` tokens
- **Concurrency**: Runs N tasks in parallel (default: 10)
- **Monotonic timing**: Uses `performance.now()` to avoid clock drift
- **Jitter**: Random 0-5ms delay to prevent thundering herd

### 2. RequestOrchestrator (`utils/RequestOrchestrator.ts`)

High-level API for managing rate-limited requests

**Key Features**:

- Single global rate limit across ALL external calls (Anthropic, Spotify, Last.fm)
- Micro-batching: Collects multiple `enqueue()` calls before processing
- Batch management: Can await specific named batches
- Per-task promises: Each task gets its own promise that resolves independently

**API**:

```typescript
// Execute single task
const result = await orchestrator.execute(() => apiCall())

// Execute batch with ID
orchestrator.enqueueBatch('my-batch', [
  () => fetch('url1'),
  () => fetch('url2'),
  () => fetch('url3'),
])
const results = await orchestrator.awaitBatch('my-batch')

// Get pending task count
const count = orchestrator.getPendingCount()
```

### 3. Rate-Limited API Wrappers (`utils/RateLimitedAPIClients.ts`)

Convenience wrappers for common API types

```typescript
// Anthropic (Claude + Haiku)
await rateLimitedAnthropicCall(() =>
  anthropic.messages.create({...}), logger, 'context'
);

// Spotify
await rateLimitedSpotifyCall(() =>
  spotify.search(query), logger, 'search tracks'
);

// Last.fm
await rateLimitedLastFmCall(() =>
  lastfm.getTrackInfo(track), logger, 'track info'
);

// Batch helper
const results = await executeBatch([
  () => spotify.search('query1'),
  () => spotify.search('query2'),
  () => spotify.search('query3')
], 'spotify-searches');
```

## Architecture Patterns

### Pattern 1: Single Request

For one-off API calls:

```typescript
import { rateLimitedSpotifyCall } from '../utils/RateLimitedAPIClients'

const playlist = await rateLimitedSpotifyCall(
  () => spotifyApi.createPlaylist(userId, { name }),
  logger,
  'create playlist',
)
```

### Pattern 2: Parallel Batch

For independent parallel requests (e.g., enriching 50 tracks):

```typescript
import { getGlobalOrchestrator } from '../utils/RateLimitedAPIClients'

const orchestrator = getGlobalOrchestrator()

// Enqueue all tasks
orchestrator.enqueueBatch(
  'enrich-tracks',
  tracks.map(track => () => lastfm.getTrackInfo(track)),
)

// Do other work here (non-blocking)

// Wait for batch when needed
const results = await orchestrator.awaitBatch('enrich-tracks')
```

### Pattern 3: Dependency Chains

For sequential dependencies with pipelined sub-tasks:

```typescript
// Step 1: Search for tracks (batched)
orchestrator.enqueueBatch(
  'searches',
  queries.map(q => () => spotify.search(q)),
)
const searchResults = await orchestrator.awaitBatch('searches')

// Step 2: Get audio features for found tracks (batched)
orchestrator.enqueueBatch(
  'audio-features',
  trackIds.map(id => () => spotify.getAudioFeatures(id)),
)
const features = await orchestrator.awaitBatch('audio-features')

// Step 3: Enrich with Last.fm (batched)
orchestrator.enqueueBatch(
  'lastfm-enrich',
  tracks.map(t => () => lastfm.getTrackInfo(t)),
)
const enriched = await orchestrator.awaitBatch('lastfm-enrich')
```

### Pattern 4: Pipeline with Progress Updates

Combine rate-limited requests with non-blocking SSE writes:

```typescript
import { SSEWriter } from './chat-stream'

// Start batch processing
orchestrator.enqueueBatch('enrich', tasks)

// Update progress without blocking (fire-and-forget)
sseWriter.writeAsync({ type: 'thinking', data: 'Enriching tracks...' })

// Process batch with progress callbacks
const results = await orchestrator.awaitBatch('enrich')

// Flush SSE writes before next phase
await sseWriter.flush()

sseWriter.writeAsync({ type: 'thinking', data: 'Analyzing features...' })
```

## SSE Write Pipeline Pattern

The `SSEWriter` class supports **non-blocking writes** with strategic flush points.

### SSEWriter API

```typescript
class SSEWriter {
  // BLOCKING: Waits for write to complete
  async write(event: StreamEvent): Promise<void>

  // NON-BLOCKING: Queues write and returns immediately
  writeAsync(event: StreamEvent): void

  // BLOCKING: Waits for all queued writes to complete
  async flush(): Promise<void>
}
```

### When to use `writeAsync()` vs `write()`

**Use `writeAsync()` (non-blocking)**:

- Progress updates during batch processing
- Thinking messages
- Non-critical debug/log messages
- Narrator-generated messages (dynamic progress)

**Use `write()` (blocking)**:

- Tool execution results (must arrive before next step)
- Error messages
- Final completion message
- Messages that must arrive before response closes

**Use `flush()` (checkpoint)**:

- Before expensive operations (ensure user sees progress)
- After batch completion (ensure all updates delivered)
- Before sending final response
- At logical phase boundaries

### Example: Pipelined Playlist Analysis

```typescript
// Phase 1: Search tracks
sseWriter.writeAsync({ type: 'thinking', data: 'ðŸ” Searching for tracks...' })

orchestrator.enqueueBatch('searches', searchTasks)
const tracks = await orchestrator.awaitBatch('searches')

// Flush before next phase (checkpoint)
await sseWriter.flush()

// Phase 2: Enrich tracks (with progress updates)
sseWriter.writeAsync({ type: 'thinking', data: 'ðŸŽ§ Enriching tracks...' })

orchestrator.enqueueBatch('enrich-tracks', enrichTasks)

// Update progress during enrichment (non-blocking)
let enriched = 0
for (const track of tracks) {
  orchestrator
    .execute(() => enrichTrack(track))
    .then(() => {
      enriched++
      if (enriched % 10 === 0) {
        sseWriter.writeAsync({
          type: 'thinking',
          data: `ðŸŽ§ Enriched ${enriched}/${tracks.length} tracks...`,
        })
      }
    })
}

const results = await orchestrator.awaitBatch('enrich-tracks')

// Flush before final response
await sseWriter.flush()

// Send final result (blocking)
await sseWriter.write({ type: 'done', data: null })
```

## Rate Limit Budget

Cloudflare Workers Free tier: **40 RPS** outbound subrequests

Typical playlist analysis flow:

- 1x Initial Claude call
- 5-10x Spotify searches
- 50x Audio features (batched)
- 50x Last.fm track info (batched)
- 45x Last.fm artist info (batched)
- 5-10x Progress narrator calls (Haiku)
- 1x Final Claude call
- 1x Create playlist
- 1-2x Add tracks

**Total**: ~170 requests over ~10 seconds = **~17 RPS average**

With bursts:

- 50 audio features in 1 second = 50 RPS burst
- Token bucket allows burst up to 40 tokens
- Remaining 10 requests queued and processed over next 0.25s

## Migration Guide

### Before (Direct API calls)

```typescript
// âŒ No rate limiting, no batching
const track1 = await lastfm.getTrackInfo(track1)
const track2 = await lastfm.getTrackInfo(track2)
const track3 = await lastfm.getTrackInfo(track3)

// âŒ Blocking SSE writes
await sseWriter.write({ type: 'thinking', data: 'Processing...' })
await sseWriter.write({ type: 'thinking', data: 'Still processing...' })
```

### After (Orchestrated + Pipelined)

```typescript
// âœ… Batched + rate limited
const orchestrator = getGlobalOrchestrator()
orchestrator.enqueueBatch('tracks', [
  () => lastfm.getTrackInfo(track1),
  () => lastfm.getTrackInfo(track2),
  () => lastfm.getTrackInfo(track3),
])

// âœ… Non-blocking SSE writes
sseWriter.writeAsync({ type: 'thinking', data: 'Processing...' })
sseWriter.writeAsync({ type: 'thinking', data: 'Still processing...' })

// âœ… Await batch when needed
const results = await orchestrator.awaitBatch('tracks')

// âœ… Flush at checkpoint
await sseWriter.flush()
```

## Performance Benefits

1. **Reduced latency**: Parallel execution of independent requests
2. **Better throughput**: Token bucket smooths request rate
3. **Improved UX**: Non-blocking SSE writes keep UI responsive
4. **Rate limit compliance**: Automatic pacing prevents 429 errors
5. **Resource efficiency**: Bounded concurrency prevents overwhelming APIs

## Monitoring

Track orchestrator performance:

```typescript
const orchestrator = getGlobalOrchestrator()

// Before batch
const startCount = orchestrator.getPendingCount()
console.log(`Starting with ${startCount} pending tasks`)

// After batch
orchestrator.enqueueBatch('my-batch', tasks)
const afterCount = orchestrator.getPendingCount()
console.log(`Now ${afterCount} pending tasks`)

// After completion
const results = await orchestrator.awaitBatch('my-batch')
const endCount = orchestrator.getPendingCount()
console.log(`Completed, ${endCount} tasks remaining`)
```

## Best Practices

1. **Always use orchestrator for external calls** - Never call APIs directly
2. **Batch independent requests** - Use `enqueueBatch()` + `awaitBatch()`
3. **Pipeline SSE writes** - Use `writeAsync()` + strategic `flush()`
4. **Use descriptive batch IDs** - Makes debugging easier
5. **Flush before expensive operations** - Ensures user sees progress
6. **Log orchestrator state** - Track pending count for debugging
7. **Handle null results** - Rate-limited calls can fail, always check for null

## Debugging

Enable detailed logging:

```typescript
import { ServiceLogger } from '../utils/ServiceLogger'

const logger = new ServiceLogger('Orchestrator')

await rateLimitedSpotifyCall(
  () => spotify.search(query),
  logger, // â† Logs timing and errors
  'search tracks',
)
```

Check orchestrator state:

```typescript
const pending = orchestrator.getPendingCount()
if (pending > 100) {
  logger.warn(`High pending count: ${pending} tasks queued`)
}
```

## Future Enhancements

1. **Adaptive rate limiting**: Adjust rate based on 429 responses
2. **Priority queues**: High-priority requests jump queue
3. **Request deduplication**: Cache identical concurrent requests
4. **Retry with backoff**: Automatic retry for transient failures
5. **Per-API rate limits**: Different limits for different APIs
6. **Metrics export**: Export queue stats for monitoring
